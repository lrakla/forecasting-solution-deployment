{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines the modeling workflow I used to solve this challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the given EDA (notebooks/eda1.ipynb), I got a sense of the features in the dataset.\n",
    "Since the series were not of the same length, it was essential to take a model which could handle that.\n",
    "Each store-dept combination would have different amount of data to learn from\n",
    "\n",
    "The sales were skewed, with some weeks having very huge sale numbers (the sales were also skewed for certain departments)\n",
    "This made me create a configurable option to log transform the weekly_sales\n",
    "\n",
    "The ACF plot indicated there is +ve correlation upto 5 weeks, but the weeks after are not significant.\n",
    "Temperature, Fuel_price, Marrkdown 2,3,4 were removed as they were not highly correlated with fuel price. However, correlation only measures a linear relationship, so the flexibility to add these features was given through a CONFIG.yaml file.\n",
    "\n",
    "Markdown 4 is highly correlated with markdown 1, so it was dropped.\n",
    "\n",
    "Store size was highly correlated with sales, so it seems that is an important feature (understandably,larger stores have more sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to do some extra exploratory analysis to identify data quality. It was seen that 0.23 % of sales are negative. I converted those to 1 so they could log transform to zero. The markdowns had a lot of missing values, so I imputed them with zero (to nullify their effect). But this imputation would be more informed if given the implication of the markdown column.\n",
    "\n",
    "Summary statistics of numeric columns\n",
    "![](figures/describe.png)\n",
    "\n",
    "Since the sales are on different scales for each store and dept, I scaled them in a MinMax fashion. \n",
    "This helped me analyse their trends and seasonality. \n",
    "The data did not have a significant trend, but was very seasonal. The last month (december) recorded high sales across all years\n",
    "\n",
    "Store sales\n",
    "![Store sales](figures/store_sales.png)\n",
    "Department sales\n",
    "![Random departments for a random store](figures/department_sales.png)\n",
    "Trends and Seasonality\n",
    "![Trends](figures/trend_seasonality.png)\n",
    "Sales per year\n",
    "![Sales per year](figures/sales_per_year.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plots, it seems while all stores have a similar pattern, each department is very unique. \n",
    "So department would be very important to forecast the sales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "Data is prepared sequentially, with data being preprocessed, then creating various features and splitting the dataset into train and test.\n",
    "The numeric columns are on different scales, so they are standardized ensuring that there is no data leakage between train and test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Here, I mainly focused on encoding the features which were categorical. I created time based features like year, month, week number. I also added lag features to the model so it can be used in a supervised setting (user can control number of lags from config file)\n",
    "\n",
    "However, this would require me to recursively add lags at each predicted time step (Current prediction would be a lag for the next prediction). I left that due to time constraints\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and evaluation\n",
    "I unfortunately was not able to experiment as much as I'd have liked. I saw the data would be need to be processed in different ways to use models like VARIMA, but it is not as powerful (I was not sure if the generated time series followed the ARIMA process, or if there are linear dependencies ) and I wanted to use the continuous and categorical features (exogenous). Hiearchical forecasting was also on my list to try, but I was not sure about the results or implementation.\n",
    "Rather than try models which may or may not work, I focused on XGBoost which would be able to handle the differing scales and types of features. It was also robust to seasonality\n",
    "I added the time based features to convert it to a supervised problem.\n",
    "\n",
    "I also experimented with DeepAR from PyTorch Forecasting, and was able to get it running. However, the results were not that strong (see implementation in notebooks/deepar.py) and I did not have time to investigate. However, I believe it is a strong candidate model for this problem as it is designed for multiple time series and used recurrent neural nets\n",
    "\n",
    "\n",
    "For evaluation, I used MAPE, Symmetric MAPE and WMAPE. The reason for using them are for their advantages outlined in this [post](https://medium.com/@vinitkothari.24/time-series-evaluation-metrics-mape-vs-wmape-vs-smape-which-one-to-use-why-and-when-part1-32d3852b4779) \n",
    "\n",
    "The results of my hyperparamter tuning are in notebooks/hyperparameter.ipynb. Squared error was used as the objective function for the model.\n",
    "\n",
    "Final results for evaluation - {'mape': 12.54, 'smape': 0.103, 'wmape': 0.113}\n",
    "\n",
    "Qualitative evaluation\n",
    "\n",
    "Summed forecasts for all stores and departments\n",
    "![](figures/summed_forecasts.png)\n",
    "\n",
    "\n",
    "Forecast for store 1\n",
    "![](figures/1.png)\n",
    "\n",
    "\n",
    "Forecast for store 4 department 3\n",
    "![](figures/1-3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "The pipeline is very streamlined, it prints and saves the forecasts for the 3 week period by running main.py. The store and department number can be passed as command line arguments to print specific forecasts.\n",
    "\n",
    "All the flexibility is included in CONFIG.yaml where the features, lags, train_split_date, forecast_create_date, transform_target etc can be controlled. \n",
    "\n",
    "I also was not sure how to include forecast_create_date, but in general the test set would be created to include dates greater than train_split_date and would be less than or equal to forecast_create_date.\n",
    "\n",
    "There are many ways this could be optimized but I refrained due to time constraints\n",
    "1. Create a checkpoint folder for version control and save model params as metadata\n",
    "2. Create a cmd argument to use a specific model for the forecasts\n",
    "3. Use MLFlow tracking to record experiments\n",
    "4. Automatically pick parameters form the best model rather than model_config.yaml\n",
    "5. Create an endpoint for serving the model predictions given data. This could include a feature store which would take in the provided features (like store, department), calculate the others from databases (size, type of store) and obtain others from different apis (CPI, unemployment etc.)\n",
    "\n",
    "\n",
    "As for well engineered code, I missed out on documenting and including asserts/tests for all functions. But this would be done ideally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Comments\n",
    "\n",
    "All in all, I had a lot of fun in this assesment. I did my best given the time, but I could definitely identify areas of improvement. It was a challenging problem to think and implement, so it helped me understand the complexities of the role."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
